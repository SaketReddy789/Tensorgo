{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7831039,"sourceType":"datasetVersion","datasetId":4589458},{"sourceId":7832694,"sourceType":"datasetVersion","datasetId":4590688},{"sourceId":7841309,"sourceType":"datasetVersion","datasetId":4596983}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision\n!pip install opencv-python","metadata":{"execution":{"iopub.status.busy":"2024-03-14T09:37:17.317078Z","iopub.execute_input":"2024-03-14T09:37:17.317849Z","iopub.status.idle":"2024-03-14T09:37:41.419544Z","shell.execute_reply.started":"2024-03-14T09:37:17.317806Z","shell.execute_reply":"2024-03-14T09:37:41.418395Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.15.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.23.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (4.8.0.76)\nRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python) (1.23.5)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Pre-trained weights of model FSRCNN_model.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass FSRCNN_model(nn.Module):\n    def __init__(self, scale: int) -> None:\n        super(FSRCNN_model, self).__init__()\n\n        if scale not in [2, 3, 4]:\n            raise ValueError(\"must be 2, 3 or 4\")\n\n        d = 56\n        s = 12\n\n        self.feature_extract = nn.Conv2d(in_channels=3, out_channels=d, kernel_size=5, padding=2)\n        nn.init.kaiming_normal_(self.feature_extract.weight)\n        nn.init.zeros_(self.feature_extract.bias)\n\n        self.activation_1 = nn.PReLU(num_parameters=d)\n\n        self.shrink = nn.Conv2d(in_channels=d, out_channels=s, kernel_size=1)\n        nn.init.kaiming_normal_(self.shrink.weight)\n        nn.init.zeros_(self.shrink.bias)\n\n        self.activation_2 = nn.PReLU(num_parameters=s)\n        \n        # m = 4\n        self.map_1 = nn.Conv2d(in_channels=s, out_channels=s, kernel_size=3, padding=1)\n        nn.init.kaiming_normal_(self.map_1.weight)\n        nn.init.zeros_(self.map_1.bias)\n\n        self.map_2 = nn.Conv2d(in_channels=s, out_channels=s, kernel_size=3, padding=1)\n        nn.init.kaiming_normal_(self.map_2.weight)\n        nn.init.zeros_(self.map_2.bias)\n\n        self.map_3 = nn.Conv2d(in_channels=s, out_channels=s, kernel_size=3, padding=1)\n        nn.init.kaiming_normal_(self.map_3.weight)\n        nn.init.zeros_(self.map_3.bias)\n\n        self.map_4 = nn.Conv2d(in_channels=s, out_channels=s, kernel_size=3, padding=1)\n        nn.init.kaiming_normal_(self.map_4.weight)\n        nn.init.zeros_(self.map_4.bias)\n\n        self.activation_3 = nn.PReLU(num_parameters=s)\n\n        self.expand = nn.Conv2d(in_channels=s, out_channels=d, kernel_size=1)\n        nn.init.kaiming_normal_(self.expand.weight)\n        nn.init.zeros_(self.expand.bias)\n\n        self.activation_4 = nn.PReLU(num_parameters=d)\n\n        self.deconv = nn.ConvTranspose2d(in_channels=d, out_channels=3, kernel_size=9, \n                                        stride=scale, padding=4, output_padding=scale-1)\n        nn.init.normal_(self.deconv.weight, mean=0.0, std=0.001)\n        nn.init.zeros_(self.deconv.bias)\n\n    def forward(self, X_in):\n        X = self.feature_extract(X_in)\n        X = self.activation_1(X)\n\n        X = self.shrink(X)\n        X = self.activation_2(X)\n\n        X = self.map_1(X)\n        X = self.map_2(X)\n        X = self.map_3(X)\n        X = self.map_4(X)\n        X = self.activation_3(X)\n\n        X = self.expand(X)\n        X = self.activation_4(X)\n\n        X = self.deconv(X)\n        X_out = torch.clip(X, 0.0, 1.0)\n\n        return X_out","metadata":{"execution":{"iopub.status.busy":"2024-03-14T09:38:58.246334Z","iopub.execute_input":"2024-03-14T09:38:58.246806Z","iopub.status.idle":"2024-03-14T09:38:58.266007Z","shell.execute_reply.started":"2024-03-14T09:38:58.246750Z","shell.execute_reply":"2024-03-14T09:38:58.264735Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Deployment","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = FSRCNN_model(scale=2).to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/input/fsrcnn-pre-trained-model-for-upscaling/FSRCNN-x2.pt\", map_location=device))","metadata":{"execution":{"iopub.status.busy":"2024-03-14T09:39:15.416704Z","iopub.execute_input":"2024-03-14T09:39:15.417073Z","iopub.status.idle":"2024-03-14T09:39:15.435827Z","shell.execute_reply.started":"2024-03-14T09:39:15.417046Z","shell.execute_reply":"2024-03-14T09:39:15.434930Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"import cv2\nimport torch\nimport numpy as np\nimport time  # Import the time module\n\ndef upscale_video(input_path, output_path, model, device, scale_factor=2):\n    start_time = time.time()  # Record the start time\n\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(\"Error opening video stream or file\")\n        return\n\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) * scale_factor)\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) * scale_factor)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frame = frame.transpose((2, 0, 1))\n        frame = np.ascontiguousarray(frame)\n\n        input_tensor = torch.tensor(frame, dtype=torch.float32).unsqueeze(0).to(device) / 255.0\n\n        with torch.no_grad():\n            output_tensor = model(input_tensor)\n\n        output_frame = output_tensor.squeeze().cpu().numpy()\n        output_frame = output_frame.transpose((1, 2, 0))\n        output_frame = (output_frame * 255.0).clip(0, 255).astype(np.uint8)\n        output_frame = cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR)\n\n        out.write(output_frame)\n\n    cap.release()\n    out.release()\n\n    end_time = time.time()  # Record the end time\n    time_taken = end_time - start_time  # Calculate the time taken\n\n    print(f\"Time taken to upscale the video: {time_taken:.2f} seconds\")\n\n\ninput_video_path = \"/kaggle/input/480p-video/videoplayback.mp4\"\noutput_video_path = \"/kaggle/working/finaloutput.mp4\"\nupscale_video(input_video_path, output_video_path, model, device, scale_factor=2)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T09:39:50.585765Z","iopub.execute_input":"2024-03-14T09:39:50.586817Z","iopub.status.idle":"2024-03-14T09:40:13.214668Z","shell.execute_reply.started":"2024-03-14T09:39:50.586743Z","shell.execute_reply":"2024-03-14T09:40:13.213692Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Time taken to upscale the video: 22.61 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"code","source":"import cv2\n\ndef get_video_resolution(video_path):\n    # Open the video file\n    cap = cv2.VideoCapture(video_path)\n    \n    if not cap.isOpened():\n        return \"Error: Could not open video.\"\n\n    # Read the video's width and height\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    \n    # Close the video file\n    cap.release()\n\n    # Match the height to a video quality standard\n    if height >= 2160:\n        return \"2160p (4K)\"\n    elif height >= 1440:\n        return \"1440p (QHD)\"\n    elif height >= 1080:\n        return \"1080p (Full HD)\"\n    elif height >= 720:\n        return \"720p (HD)\"\n    elif height >= 480:\n        return \"480p (SD)\"\n    else:\n        return \"Lower than 480p\"\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T09:40:22.851463Z","iopub.execute_input":"2024-03-14T09:40:22.851927Z","iopub.status.idle":"2024-03-14T09:40:22.859429Z","shell.execute_reply.started":"2024-03-14T09:40:22.851888Z","shell.execute_reply":"2024-03-14T09:40:22.858323Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Input Video's resolution: ","metadata":{}},{"cell_type":"code","source":"video_path = \"/kaggle/input/480p-video/videoplayback.mp4\"\nvideo_quality = get_video_resolution(video_path)\nprint(f\"The video quality is: {video_quality}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-14T09:40:38.080422Z","iopub.execute_input":"2024-03-14T09:40:38.080814Z","iopub.status.idle":"2024-03-14T09:40:38.092994Z","shell.execute_reply.started":"2024-03-14T09:40:38.080768Z","shell.execute_reply":"2024-03-14T09:40:38.091993Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"The video quality is: 480p (SD)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Output video's resolution","metadata":{}},{"cell_type":"code","source":"video_path = \"/kaggle/working/finaloutput.mp4\"\nvideo_quality = get_video_resolution(video_path)\nprint(f\"The video quality is: {video_quality}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-14T09:40:48.319654Z","iopub.execute_input":"2024-03-14T09:40:48.320024Z","iopub.status.idle":"2024-03-14T09:40:48.326486Z","shell.execute_reply.started":"2024-03-14T09:40:48.319995Z","shell.execute_reply":"2024-03-14T09:40:48.325442Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"The video quality is: 720p (HD)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}